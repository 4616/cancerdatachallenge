{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge\n",
    "\n",
    "Top line summary: variables are HIGHLY predictive of whether a tumor is malignant or benign. I achieve 98% accuracy, percision, and recall for an out of sample test set.\n",
    "\n",
    "## Data cleaning\n",
    "I droped the few observations with missing data. A small number of patients have multiple samples. In these instances, I only use the first sample in the dataset so that information does not leak between the train and test sets.\n",
    "\n",
    "## Feature Engineering\n",
    "In this instance, there was no need to engineer features because the given features were already highly predictive. However, all of these features appear to require that a human being look at a cell. As the original paper describes, \"characteristics of breast fine-needle aspirates reported to differ between benign and malignant samples was graded 1 to 10 at the time of sample collection.\" So the task in this data challenge is to predict one human assessment of cells with another human assessment of cells. Thus, the model trained here would still require expert human input. In future work a deep learning mode could be used predict the features in this dataset from images to help auto engineer these features.\n",
    "## Machine learning\n",
    "\n",
    "I use an ensemble of machine learning algorithms weighted by their out of sample performance in cross validation to predict whether a tumor is malignant (cancer!). Cancer is bad, failing to detect cancer is really bad (false negative). We also want to avoid telling someone they have cancer when they are in remission or cancer-free (false positive). Even though I achieve 98% recall and 98% precision, it still may make sense to have a human being double check the results.\n",
    "\n",
    "## Human Learning\n",
    "\n",
    "The features in the dataset were picked by experts who thought they would be predictive of cancer. Collectively the nine features are highly predictive, but we can also break down the different features to see which individual features are predictive and which aren't. The only feature that is not a statistically significant predictor of cancer mitoses, all other features are significant in a regression. Thus, these results indicate that the researchers who constructed the dataset had a good understanding of what cell features are indicators for cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdougal/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#This is a block of prior code that defines an ensemble of machine learning algorithms \n",
    "#weighted by their out of sample performance in cross validation\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoLars\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor \n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "#import statsmodels.api as sm\n",
    "\n",
    "\n",
    "mset1 = [LogisticRegression(), Lasso(alpha = .1), \n",
    "      GaussianNB(), MultinomialNB(), SVC(), DecisionTreeClassifier(), \n",
    "        DecisionTreeRegressor(), RandomForestClassifier(), \n",
    "         RandomForestRegressor(), Ridge()]\n",
    "\n",
    "\n",
    "modelsfast = [LogisticRegression(), Lasso(alpha = .1), \n",
    "                    RandomForestClassifier(), GaussianNB(), MultinomialNB()]\n",
    "\n",
    "models = modelsfast + [SVC(), DecisionTreeClassifier(), DecisionTreeRegressor()]\n",
    "\n",
    "modelsplus = [Ridge(), LinearRegression(), AdaBoostClassifier(), RandomForestRegressor()]\n",
    "\n",
    "\n",
    "modelsfull = models + [Ridge(), GradientBoostingClassifier(), GradientBoostingRegressor(), LinearRegression(), AdaBoostClassifier(), RandomForestRegressor()]\n",
    "\n",
    "modelsfullCV = modelsfull + [ElasticNetCV(), LassoCV()]\n",
    "def meansquarederror(y, y_hat):\n",
    "    error = (y - y_hat)** 2\n",
    "    return error.mean()\n",
    "       \n",
    "\n",
    "def roundinbounds(y, lower, upper):\n",
    "    y = int(round(y))\n",
    "    if y >= lower:\n",
    "        if y <= upper:\n",
    "            return y\n",
    "        else:\n",
    "            return upper\n",
    "    \n",
    "    else:\n",
    "        return lower\n",
    "        \n",
    "          \n",
    "\n",
    "def zeroone(y):\n",
    "    if y >= .5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "def numboolean(x, y):\n",
    "    if x == y:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def fitfoldk(traintest):\n",
    "    \n",
    "    train = traintest[0]\n",
    "    test = traintest[1]\n",
    "    v = traintest[2]\n",
    "    models = traintest[3]\n",
    "    X = traintest[4]\n",
    "    Y = traintest[5]\n",
    "    \n",
    "    del traintest\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = X[train], X[test], Y[train], Y[test]\n",
    "    i = 0\n",
    "    for model in models:\n",
    "        start = time.clock()\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "            \n",
    "        y_hat = model.predict(X_test).reshape(-1,1)\n",
    "        if i == 0:\n",
    "            y_hats_k = y_hat\n",
    "\n",
    "        if i != 0:\n",
    "            y_hats_k = np.concatenate((y_hats_k, y_hat), axis = 1)\n",
    "       \n",
    "\n",
    "        modeltime = time.clock() - start\n",
    "        print(v, i, modeltime)\n",
    "        i += 1\n",
    "\n",
    "    return y_hats_k\n",
    "\n",
    "\n",
    "def fitfoldkmodeli(traintest):\n",
    "    \n",
    "    train = traintest[0]\n",
    "    test = traintest[1]\n",
    "    v = traintest[2]\n",
    "    i = traintest[3]\n",
    "    model = traintest[4]\n",
    "    X = traintest[5]\n",
    "    Y = traintest[6]\n",
    "    \n",
    "    del traintest\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = X[train], X[test], Y[train], Y[test]\n",
    "\n",
    "\n",
    "    start = time.clock()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "    y_hat = model.predict(X_test).reshape(-1,1)\n",
    "\n",
    "\n",
    "    modeltime = time.clock() - start\n",
    "    print(v, i, modeltime)\n",
    "\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def predictfoldx(args):\n",
    "    fittedmodels = args[0]\n",
    "    X = args[1]\n",
    "    SL = args[2]\n",
    "\n",
    "    i = 0\n",
    "    for model in fittedmodels:\n",
    "        y_hat = model.predict(X).reshape(-1,1)\n",
    "\n",
    "        if i == 0:\n",
    "            y_hats_k = y_hat\n",
    "        if i != 0:\n",
    "            y_hats_k = np.concatenate((y_hats_k, y_hat), axis = 1)\n",
    "\n",
    "        i+= 1\n",
    "\n",
    "    y_hat_SL = SL.predict(y_hats_k)\n",
    "\n",
    "    return y_hat_SL\n",
    "\n",
    "def predictparallel(self, X, Y = None):\n",
    "    lx = len(X)\n",
    "    start = 0\n",
    "    stop = 1000\n",
    "    \n",
    "    arglist = []\n",
    "    \n",
    "    while start < lx:\n",
    "        arglist.append([self.fittedmodels, X[start:stop]])\n",
    "\n",
    "class SuperLearner(object):\n",
    "\n",
    "    def parallelfit(self, X, Y, models = models, k = 10, n = 8):\n",
    "        self.lower = Y.min()\n",
    "        self.upper = Y.max()\n",
    "        \n",
    "\n",
    "        if k > len(Y):\n",
    "            print(\"Number of folds cannot exceed number of observations. Cross-validation reduced to n-fold cross-validation.\")\n",
    "            k = len(Y)\n",
    "\n",
    "        kf = KFold(len(Y), n_folds = k)\n",
    "        MSE = {}\n",
    "        y_actual = np.array([])\n",
    "        v = 0\n",
    "        \n",
    "        \n",
    "\n",
    "        arglist = []\n",
    "        for train, test in kf:\n",
    "            i = 0\n",
    "            y_actual = np.concatenate((y_actual, Y[test]))\n",
    "            for model in models:           \n",
    "                arglist.append([train, test, v, i, model, X, Y])\n",
    "                i += 1\n",
    "            v += 1\n",
    "            \n",
    "        pool = Pool(processes = n)\n",
    "        modelresults = pool.map(fitfoldkmodeli, arglist)\n",
    "        pool.close()\n",
    "        #working\n",
    "        \n",
    "        i = 0\n",
    "        j = 0\n",
    "        counter = 0\n",
    "        foldresults = []\n",
    "        while j < k:\n",
    "            i = 0\n",
    "\n",
    "            while i < len(models):\n",
    "\n",
    "\n",
    "                if i == 0:\n",
    "                    y_hats_k = modelresults[counter]\n",
    "    \n",
    "                if i != 0:\n",
    "                    y_hats_k = np.concatenate((y_hats_k, modelresults[counter]), axis = 1)\n",
    "\n",
    "\n",
    "                #print(i,j,counter, k, len(models), len(modelresults))\n",
    "                counter += 1      \n",
    "                i += 1\n",
    "                \n",
    "            j += 1\n",
    "            foldresults.append(y_hats_k)\n",
    "            del y_hats_k\n",
    "\n",
    "\n",
    "        \n",
    "        exists = False\n",
    "        for fold in foldresults:\n",
    "            if exists:\n",
    "                y_hats = np.concatenate((y_hats, fold))\n",
    "            \n",
    "            else:              \n",
    "                y_hats = fold\n",
    "                exists = True\n",
    "        \n",
    "            \n",
    "\n",
    "            \n",
    "        #print(\"shape\")\n",
    "        #print(y_hats.shape)  \n",
    "\n",
    "        #Estimate SuperLearner\n",
    "\n",
    "        \n",
    "        self.SL = Lasso(alpha = .001)\n",
    "        #self.SL = LinearRegression()\n",
    "        self.SL.fit(y_hats, y_actual)\n",
    "        \n",
    "        self.ols = LinearRegression()\n",
    "        self.ols.fit(y_hats, y_actual)\n",
    "\n",
    "        self.lasso = Lasso(alpha = .001)\n",
    "        self.lasso.fit(y_hats, y_actual)   \n",
    "\n",
    "        self.lassopositive = Lasso(alpha = .001, positive = True)\n",
    "        self.lassopositive.fit(y_hats, y_actual)        \n",
    "        \n",
    "        y_hat_SL = self.SL.predict(y_hats)\n",
    "\n",
    "\n",
    "\n",
    "        self.fittedmodels = []\n",
    "        for model in models:\n",
    "            model.fit(X, Y)\n",
    "            self.fittedmodels.append(model)\n",
    "\n",
    "\n",
    "    def parallelfitoriginal(self, X, Y, models = models, k = 10, n = 8):\n",
    "        self.lower = Y.min()\n",
    "        self.upper = Y.max()\n",
    "        \n",
    "\n",
    "        if k > len(Y):\n",
    "            print(\"Number of folds cannot exceed number of observations. Cross-validation reduced to n-fold cross-validation.\")\n",
    "            k = len(Y)\n",
    "\n",
    "        kf = KFold(len(Y), n_folds = k)\n",
    "        MSE = {}\n",
    "        y_actual = np.array([])\n",
    "        v = 0\n",
    "        \n",
    "\n",
    "        arglist = []\n",
    "        for train, test in kf:\n",
    "            y_actual = np.concatenate((y_actual, Y[test]))\n",
    "            arglist.append([train, test, v, models, X, Y])\n",
    "            v += 1\n",
    "            \n",
    "        pool = Pool(processes = n)\n",
    "        foldresults = pool.map(fitfoldk, arglist)\n",
    "        pool.close()\n",
    "        \n",
    "        exists = False\n",
    "        for fold in foldresults:\n",
    "            if exists:\n",
    "                y_hats = np.concatenate((y_hats, fold))\n",
    "            \n",
    "            else:              \n",
    "                y_hats = fold\n",
    "                exists = True\n",
    "        \n",
    "            \n",
    "\n",
    "        print(\"shape\")\n",
    "        print(y_hats.shape)           \n",
    "\n",
    "\n",
    "\n",
    "        #Estimate SuperLearner\n",
    "\n",
    "        self.SL = Lasso(alpha = 0)\n",
    "        self.SL.fit(y_hats, y_actual)\n",
    "        y_hat_SL = self.SL.predict(y_hats)\n",
    "\n",
    "\n",
    "\n",
    "        self.fittedmodels = []\n",
    "        for model in models:\n",
    "            model.fit(X, Y)\n",
    "            self.fittedmodels.append(model)\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, Y, models = models, k = 10):\n",
    "        self.lower = Y.min()\n",
    "        self.upper = Y.max()\n",
    "\n",
    "\n",
    "        if k > len(Y):\n",
    "            print(\"Number of folds cannot exceed number of observations. Cross-validation reduced to n-fold cross-validation.\")\n",
    "            k = len(Y)\n",
    "\n",
    "        kf = KFold(len(Y), n_folds = k)\n",
    "        MSE = {}\n",
    "        y_actual = np.array([])\n",
    "        v = 0\n",
    "\n",
    "        for train, test in kf:\n",
    "            X_train, X_test, y_train, y_test = X[train], X[test], Y[train], Y[test]\n",
    "\n",
    "            i = 0\n",
    "            \n",
    "            y_actual = np.concatenate((y_actual, y_test))\n",
    "\n",
    "            for model in models:\n",
    "                start = time.clock()\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                \n",
    "                y_hat = model.predict(X_test).reshape(-1,1)\n",
    "                if i == 0:\n",
    "                    y_hats_k = y_hat\n",
    "\n",
    "                if i != 0:\n",
    "                    y_hats_k = np.concatenate((y_hats_k, y_hat), axis = 1)\n",
    "               \n",
    "                error = (y_test - y_hat)** 2\n",
    "             \n",
    "                if i not in MSE:\n",
    "                    MSE[i] = []\n",
    "                    \n",
    "                if i in MSE:\n",
    "                    MSE[i].append(error.mean())\n",
    "\n",
    "                modeltime = time.clock() - start\n",
    "                print(v, i, modeltime)\n",
    "                i += 1\n",
    "\n",
    "            if v != 0:\n",
    "                y_hats = np.concatenate((y_hats, y_hats_k))\n",
    "\n",
    "            if v == 0:\n",
    "                y_hats = y_hats_k\n",
    "\n",
    "            v += 1\n",
    "            del y_hats_k\n",
    "\n",
    "\n",
    "        #Estimate SuperLearner\n",
    "\n",
    "        self.SL = LinearRegression() #Lasso(alpha = 0) #LinearRegression() #Lasso(alpha = 0)#LinearRegression()  \n",
    "        self.SL.fit(y_hats, y_actual)\n",
    "        y_hat_SL = self.SL.predict(y_hats)\n",
    "\n",
    "        error = (y_actual - y_hat_SL)** 2\n",
    "        MSE[i] = []\n",
    "        MSE[i].append(error.mean())    \n",
    "            \n",
    "        MSEresult = []\n",
    "        for item in MSE:\n",
    "            MSEresult.append(np.asarray(MSE[item]).mean())\n",
    "\n",
    "        self.fittedmodels = []\n",
    "        for model in models:\n",
    "            model.fit(X, Y)\n",
    "            self.fittedmodels.append(model)\n",
    "\n",
    "\n",
    "        print(MSEresult)\n",
    "\n",
    "\n",
    "    def predict(self, X, Y = None):\n",
    "\n",
    "        i = 0\n",
    "        for model in self.fittedmodels:\n",
    "            y_hat = model.predict(X).reshape(-1,1)\n",
    "\n",
    "            if i == 0:\n",
    "                y_hats_k = y_hat\n",
    "            if i != 0:\n",
    "                y_hats_k = np.concatenate((y_hats_k, y_hat), axis = 1)\n",
    "\n",
    "            if Y != None:\n",
    "                error = (Y - y_hat)** 2\n",
    "                print(\"MSE for\", type(model), error.mean())\n",
    "                \n",
    "            i+= 1\n",
    "\n",
    "        y_hat_SL = self.SL.predict(y_hats_k)\n",
    "        if Y != None:\n",
    "            error = (Y - y_hat_SL)** 2\n",
    "            print(\"MSE for SuperLearner\", error.mean())\n",
    "\n",
    "        return y_hat_SL\n",
    "    \n",
    "    def parallelpredict(self, X, Y = None):\n",
    "        lx = len(X)\n",
    "        start = 0\n",
    "        stop = 2000\n",
    "        \n",
    "        arglist = []\n",
    "        \n",
    "        while start < lx:\n",
    "            arglist.append([self.fittedmodels, X[start:stop], self.SL])\n",
    "            start = stop\n",
    "            stop += 2000\n",
    "        \n",
    "        pool = Pool(processes = multiprocessing.cpu_count())\n",
    "\n",
    "        y_hat_SL = pool.map(predictfoldx, arglist)\n",
    "        \n",
    "        pool.close()\n",
    "        time.sleep(.25)\n",
    "        y_hat_SL = np.concatenate(y_hat_SL, axis = 0)\n",
    "        \n",
    "        if Y != None:\n",
    "            error = (Y - y_hat_SL)** 2\n",
    "            print(error.mean())\n",
    "        \n",
    "        return y_hat_SL\n",
    "            \n",
    "        \n",
    "\n",
    "    def classify(self, y_hat_SL, y_actual = None):\n",
    "        classifier = np.vectorize(roundinbounds)\n",
    "        classified = classifier(y_hat_SL, self.lower, self.upper)\n",
    "        if y_actual is not None:\n",
    "            right = np.vectorize(numboolean)(classified, y_actual)\n",
    "            print(\"Classification accuracy of \" + str(right.mean()))\n",
    "            return classified, right\n",
    "        \n",
    "        return classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test on toy dataset to make sure it's working correctly            \n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data \n",
    "Y_iris = iris.target\n",
    "\n",
    "X_train_iris = X_iris[:120]\n",
    "X_test_iris = X_iris[121:]\n",
    "Y_train_iris = Y_iris[:120]\n",
    "Y_test_iris = Y_iris[121:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5, 0.0027999999999999995)\n",
      "(0, 0, 0.005199999999999999)\n",
      "(0, 6, 0.0009129999999999997)\n",
      "(0, 7, 0.00047000000000000167)\n",
      "(0, 1, 0.0012790000000000006)\n",
      "(2, 3, 0.002501)\n",
      "(0, 10, 0.03459)\n",
      "(1, 4, 0.005832)\n",
      "(2, 4, 0.005015)\n",
      "(1, 5, 0.0018770000000000002)\n",
      "(2, 5, 0.0024739999999999988)\n",
      "(1, 6, 0.0007989999999999976)\n",
      "(2, 6, 0.0005820000000000027)\n",
      "(1, 7, 0.00034500000000000156)\n",
      "(2, 7, 0.00046500000000000014)\n",
      "(1, 8, 0.0022889999999999994)\n",
      "(0, 8, 0.0038989999999999997)\n",
      "(2, 8, 0.0015549999999999974)\n",
      "(0, 2, 0.057165)\n",
      "(0, 3, 0.0015080000000000093)\n",
      "(0, 11, 0.017386)\n",
      "(0, 15, 0.077325)\n",
      "(1, 0, 0.0021700000000000053)\n",
      "(1, 14, 0.07652)\n",
      "(1, 1, 0.0008840000000000098)\n",
      "(0, 4, 0.0028530000000000083)\n",
      "(2, 13, 0.073203)\n",
      "(1, 15, 0.070706)\n",
      "(1, 9, 0.28092700000000004)\n",
      "(1, 2, 0.078963)\n",
      "(1, 3, 0.001707000000000014)\n",
      "(3, 7, 0.0004689999999999972)\n",
      "(2, 0, 0.0020630000000000093)\n",
      "(3, 8, 0.0012899999999999856)\n",
      "(2, 1, 0.0007759999999999989)\n",
      "(3, 2, 0.072786)\n",
      "(2, 14, 0.07321199999999999)\n",
      "(1, 10, 0.03358500000000003)\n",
      "(2, 9, 0.26261999999999996)\n",
      "(3, 3, 0.001558999999999977)\n",
      "(3, 4, 0.001479000000000008)\n",
      "(3, 5, 0.0013120000000000076)\n",
      "(3, 6, 0.0006740000000000079)\n",
      "(2, 10, 0.031914)\n",
      "(1, 11, 0.036960999999999966)\n",
      "(0, 12, 0.5510400000000001)\n",
      "(2, 15, 0.06065599999999999)\n",
      "(3, 0, 0.001565999999999984)\n",
      "(2, 11, 0.007370999999999961)\n",
      "(3, 1, 0.0007240000000000024)\n",
      "(4, 1, 0.0008110000000000062)\n",
      "(0, 9, 0.23374200000000003)\n",
      "(2, 2, 0.076154)\n",
      "(4, 6, 0.0005620000000000069)\n",
      "(4, 7, 0.00042599999999998195)\n",
      "(4, 11, 0.0038519999999999943)\n",
      "(4, 8, 0.0009779999999999789)\n",
      "(0, 13, 0.067249)\n",
      "(3, 9, 0.267031)\n",
      "(4, 2, 0.07524)\n",
      "(3, 10, 0.03377599999999997)\n",
      "(4, 3, 0.0016639999999999988)\n",
      "(3, 11, 0.01724399999999998)\n",
      "(4, 4, 0.0014659999999999673)\n",
      "(4, 5, 0.0012700000000000489)\n",
      "(5, 0, 0.0036890000000000533)\n",
      "(0, 14, 0.07156099999999999)\n",
      "(5, 1, 0.002130999999999994)\n",
      "(5, 5, 0.0011900000000000244)\n",
      "(5, 6, 0.0006789999999999852)\n",
      "(5, 7, 0.0005060000000000064)\n",
      "(5, 8, 0.0009870000000000156)\n",
      "(5, 2, 0.17961500000000008)\n",
      "(5, 3, 0.0038279999999999426)\n",
      "(5, 4, 0.004809999999999981)\n",
      "(5, 10, 0.026912999999999965)\n",
      "(2, 12, 0.501201)\n",
      "(5, 11, 0.0015560000000000018)\n",
      "(1, 12, 0.489914)\n",
      "(6, 4, 0.001182999999999934)\n",
      "(5, 15, 0.19678799999999996)\n",
      "(3, 12, 0.221846)\n",
      "(6, 5, 0.0011320000000000219)\n",
      "(6, 0, 0.004462999999999995)\n",
      "(6, 6, 0.0005819999999999714)\n",
      "(6, 1, 0.0008400000000000629)\n",
      "(6, 7, 0.00045200000000000795)\n",
      "(6, 8, 0.0008060000000000844)\n",
      "(4, 12, 0.423336)\n",
      "(3, 13, 0.069571)\n",
      "(4, 9, 0.24722499999999997)\n",
      "(1, 13, 0.07440399999999991)\n",
      "(4, 10, 0.026790999999999898)\n",
      "(6, 2, 0.06770599999999993)\n",
      "(6, 3, 0.0016019999999998813)\n",
      "(7, 3, 0.0013279999999999959)\n",
      "(7, 4, 0.0019179999999999753)\n",
      "(7, 8, 0.0009410000000000807)\n",
      "(7, 5, 0.0012139999999999374)\n",
      "(5, 9, 0.23074000000000006)\n",
      "(7, 6, 0.0007080000000000419)\n",
      "(7, 7, 0.0004750000000000032)\n",
      "(4, 13, 0.071469)\n",
      "(3, 14, 0.07231400000000004)\n",
      "(6, 14, 0.072847)\n",
      "(5, 12, 0.587402)\n",
      "(7, 13, 0.06950599999999996)\n",
      "(8, 2, 0.07599699999999998)\n",
      "(4, 14, 0.07199599999999995)\n",
      "(8, 3, 0.001618999999999926)\n",
      "(8, 4, 0.0014250000000000096)\n",
      "(8, 5, 0.0011149999999999771)\n",
      "(6, 15, 0.07114799999999999)\n",
      "(5, 13, 0.05743699999999996)\n",
      "(7, 0, 0.0018089999999999495)\n",
      "(3, 15, 0.06989100000000004)\n",
      "(8, 6, 0.0006509999999999572)\n",
      "(6, 9, 0.214468)\n",
      "(7, 1, 0.0008060000000000844)\n",
      "(8, 7, 0.0004629999999999912)\n",
      "(4, 0, 0.0014810000000000656)\n",
      "(8, 8, 0.0010129999999999306)\n",
      "(7, 14, 0.07173700000000005)\n",
      "(6, 10, 0.032165)\n",
      "(4, 15, 0.06908199999999998)\n",
      "(6, 11, 0.00599100000000008)\n",
      "(9, 1, 0.0007809999999999206)\n",
      "(5, 14, 0.0707279999999999)\n",
      "(9, 6, 0.0006350000000001632)\n",
      "(9, 7, 0.0004009999999998737)\n",
      "(9, 8, 0.001150999999999902)\n",
      "(7, 2, 0.07645400000000002)\n",
      "(9, 11, 0.0022299999999999542)\n",
      "(7, 15, 0.06615000000000004)\n",
      "(8, 0, 0.0014020000000000143)\n",
      "(8, 1, 0.0008320000000000549)\n",
      "(7, 9, 0.24879399999999996)\n",
      "(9, 2, 0.07173000000000007)\n",
      "(9, 3, 0.0013769999999999616)\n",
      "(9, 4, 0.0015200000000000768)\n",
      "(9, 5, 0.0010529999999999706)\n",
      "(7, 10, 0.033162000000000136)\n",
      "(7, 11, 0.003242999999999885)\n",
      "(8, 9, 0.229981)\n",
      "(8, 12, 0.23732200000000003)\n",
      "(6, 12, 0.537928)\n",
      "(9, 12, 0.4938689999999999)\n",
      "(8, 10, 0.03182800000000008)\n",
      "(8, 13, 0.07030199999999998)\n",
      "(6, 13, 0.069388)\n",
      "(8, 11, 0.017499000000000042)\n",
      "(9, 13, 0.062562)\n",
      "(9, 9, 0.219943)\n",
      "(8, 14, 0.059979000000000005)\n",
      "(9, 10, 0.023284000000000082)\n",
      "(9, 14, 0.05147899999999983)\n",
      "(7, 12, 0.5886609999999999)\n",
      "(8, 15, 0.04950699999999986)\n",
      "(9, 0, 0.0012550000000000061)\n",
      "(9, 15, 0.04804200000000014)\n",
      "('MSE for', <class 'sklearn.linear_model.logistic.LogisticRegression'>, 0.58620689655172409)\n",
      "('MSE for', <class 'sklearn.linear_model.coordinate_descent.Lasso'>, 0.23954980235677575)\n",
      "('MSE for', <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 0.2413793103448276)\n",
      "('MSE for', <class 'sklearn.naive_bayes.GaussianNB'>, 0.068965517241379309)\n",
      "('MSE for', <class 'sklearn.naive_bayes.MultinomialNB'>, 1.0)\n",
      "('MSE for', <class 'sklearn.svm.classes.SVC'>, 0.17241379310344829)\n",
      "('MSE for', <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 0.27586206896551724)\n",
      "('MSE for', <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 0.20689655172413793)\n",
      "('MSE for', <class 'sklearn.linear_model.ridge.Ridge'>, 0.13860293323111259)\n",
      "('MSE for', <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 0.27586206896551724)\n",
      "('MSE for', <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>, 0.24090388067833657)\n",
      "('MSE for', <class 'sklearn.linear_model.base.LinearRegression'>, 0.13408310896668646)\n",
      "('MSE for', <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 0.27586206896551724)\n",
      "('MSE for', <class 'sklearn.ensemble.forest.RandomForestRegressor'>, 0.12551724137931036)\n",
      "('MSE for', <class 'sklearn.linear_model.coordinate_descent.ElasticNetCV'>, 0.13507592271533583)\n",
      "('MSE for', <class 'sklearn.linear_model.coordinate_descent.LassoCV'>, 0.13449149325689205)\n",
      "('MSE for SuperLearner', 0.077164191070685006)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdougal/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:429: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/mdougal/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:436: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "SL_iris = SuperLearner()\n",
    "SL_iris.parallelfit(X_train_iris, Y_train_iris, models = modelsfullCV, k = 10)\n",
    "y_hat_SL_iris = SL_iris.predict(X_test_iris, Y_test_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy of 0.931034482759\n",
      "[-0.39 -1.32  0.12  0.59  0.27  0.26 -0.1   0.19  3.24  0.19 -0.87 -2.86\n",
      "  0.86 -0.62  0.47  0.84]\n",
      "[-0.21 -0.21  0.12  0.49  0.1   0.21  0.    0.    0.    0.   -0.31  0.11\n",
      "  0.22  0.    0.41  0.04]\n"
     ]
    }
   ],
   "source": [
    "classified, right = SL_iris.classify(y_hat_SL_iris, Y_test_iris)\n",
    "print(SL_iris.ols.coef_.round(2))\n",
    "print(SL_iris.lasso.coef_.round(2))\n",
    "a = SL_iris.ols.coef_\n",
    "b = SL_iris.lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "1. Sample code number: id number \n",
    "2. Clump Thickness: 1 - 10 \n",
    "3. Uniformity of Cell Size: 1 - 10 \n",
    "4. Uniformity of Cell Shape: 1 - 10 \n",
    "5. Marginal Adhesion: 1 - 10 \n",
    "6. Single Epithelial Cell Size: 1 - 10 \n",
    "7. Bare Nuclei: 1 - 10 \n",
    "8. Bland Chromatin: 1 - 10 \n",
    "9. Normal Nucleoli: 1 - 10 \n",
    "10. Mitoses: 1 - 10 \n",
    "11. Class: (2 for benign, 4 for malignant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699\n",
      "683\n",
      "630\n",
      "630\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('breast-cancer-wisconsin.data', header = None)\n",
    "df.columns = ['id', 'clump_thickness', 'size_uniformity', 'shape_uniformity', 'marginal', 'single_epithelial', 'bare_nucleoli',\n",
    "               'bland_chromatin', 'normal_nucleoli', 'mitoses', 'class_type']\n",
    "df['cancer'] = (df['class_type'] - 2)/2\n",
    "print len(df)\n",
    "df = df[df.bare_nucleoli != '?'] #Drop missing\n",
    "df.bare_nucleoli = df.bare_nucleoli.astype(int) #Make column type int\n",
    "print len(df)\n",
    "print len(set(list(df.id)))\n",
    "df = df[df.duplicated(subset=['id'], keep='first') != True] #Drop duplicates\n",
    "print len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36507936507936506"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.asarray(df[['clump_thickness', 'size_uniformity', 'shape_uniformity', 'marginal', 'single_epithelial', 'bare_nucleoli',\n",
    "               'bland_chromatin', 'normal_nucleoli', 'mitoses']])\n",
    "Y = np.asarray(df['cancer'])\n",
    "Y.mean() #Not wildly imbalanced, so will not do any resampling to get even training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=432478742)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0.0031259999999999994)\n",
      "(0, 1, 0.0010099999999999987)\n",
      "(0, 5, 0.017993999999999996)\n",
      "(0, 6, 0.0010330000000000061)\n",
      "(0, 7, 0.0008080000000000032)\n",
      "(0, 8, 0.0022119999999999987)\n",
      "(1, 1, 0.0019509999999999996)\n",
      "(0, 2, 0.064261)\n",
      "(0, 3, 0.0017720000000000097)\n",
      "(1, 6, 0.002005)\n",
      "(1, 7, 0.0010469999999999993)\n",
      "(0, 10, 0.052217000000000006)\n",
      "(1, 2, 0.06415699999999999)\n",
      "(0, 4, 0.004252000000000006)\n",
      "(1, 11, 0.006137999999999999)\n",
      "(1, 8, 0.002649000000000002)\n",
      "(2, 7, 0.0014480000000000014)\n",
      "(0, 11, 0.042205000000000006)\n",
      "(1, 3, 0.0019500000000000073)\n",
      "(0, 9, 0.114815)\n",
      "(2, 8, 0.002928)\n",
      "(2, 2, 0.069646)\n",
      "(2, 3, 0.0016349999999999976)\n",
      "(1, 4, 0.011729000000000003)\n",
      "(3, 3, 0.0017749999999999988)\n",
      "(2, 4, 0.019789)\n",
      "(1, 5, 0.01949300000000001)\n",
      "(2, 5, 0.015852000000000005)\n",
      "(3, 4, 0.010130999999999973)\n",
      "(2, 6, 0.0017479999999999996)\n",
      "(3, 5, 0.019751999999999992)\n",
      "(3, 6, 0.0016770000000000118)\n",
      "(3, 7, 0.0011969999999999759)\n",
      "(4, 4, 0.009477999999999986)\n",
      "(3, 8, 0.006418000000000007)\n",
      "(4, 5, 0.026756000000000002)\n",
      "(2, 9, 0.135046)\n",
      "(4, 6, 0.0018469999999999875)\n",
      "(0, 12, 0.5713239999999999)\n",
      "(1, 9, 0.124752)\n",
      "(4, 7, 0.0018269999999999675)\n",
      "(4, 8, 0.003166000000000002)\n",
      "(3, 13, 0.10977999999999996)\n",
      "(1, 10, 0.053520999999999985)\n",
      "(4, 0, 0.003182000000000018)\n",
      "(4, 1, 0.0012719999999999954)\n",
      "(5, 0, 0.003245999999999999)\n",
      "(2, 10, 0.054674)\n",
      "(3, 9, 0.13212700000000002)\n",
      "(5, 1, 0.0012149999999999939)\n",
      "(2, 12, 0.313946)\n",
      "(0, 13, 0.06180599999999992)\n",
      "(1, 0, 0.0031649999999999734)\n",
      "(3, 10, 0.052898)\n",
      "(3, 11, 0.002460999999999991)\n",
      "(2, 11, 0.05988999999999997)\n",
      "(5, 5, 0.010653000000000024)\n",
      "(1, 12, 0.358557)\n",
      "(5, 6, 0.0013239999999999919)\n",
      "(5, 2, 0.07403299999999999)\n",
      "(5, 7, 0.0009369999999999656)\n",
      "(5, 3, 0.0017520000000000313)\n",
      "(4, 2, 0.077065)\n",
      "(5, 8, 0.001342999999999983)\n",
      "(5, 4, 0.017922999999999967)\n",
      "(1, 13, 0.06749900000000003)\n",
      "(2, 1, 0.0010529999999999706)\n",
      "(2, 0, 0.002787999999999957)\n",
      "(4, 3, 0.0014339999999999908)\n",
      "(4, 9, 0.21960999999999997)\n",
      "(6, 1, 0.0025740000000000207)\n",
      "(5, 10, 0.19032099999999996)\n",
      "(6, 6, 0.0012949999999999906)\n",
      "(5, 11, 0.0032730000000000814)\n",
      "(2, 13, 0.06914599999999993)\n",
      "(3, 0, 0.0025479999999999947)\n",
      "(6, 11, 0.005184000000000022)\n",
      "(6, 7, 0.0009240000000000359)\n",
      "(6, 8, 0.0011929999999999996)\n",
      "(3, 1, 0.0011320000000000219)\n",
      "(4, 10, 0.05003599999999997)\n",
      "(6, 2, 0.19651499999999994)\n",
      "(5, 9, 0.12314400000000003)\n",
      "(4, 11, 0.0025810000000000555)\n",
      "(6, 3, 0.0018329999999999735)\n",
      "(6, 4, 0.0021300000000000763)\n",
      "(3, 2, 0.06843600000000005)\n",
      "(7, 2, 0.06908900000000007)\n",
      "(6, 5, 0.018859999999999988)\n",
      "(7, 3, 0.0016220000000000123)\n",
      "(7, 7, 0.0009020000000000694)\n",
      "(7, 4, 0.012063999999999964)\n",
      "(7, 8, 0.0012530000000000596)\n",
      "(7, 5, 0.018445000000000045)\n",
      "(7, 6, 0.0018329999999999735)\n",
      "(8, 3, 0.002415000000000056)\n",
      "(8, 4, 0.00927800000000012)\n",
      "(6, 9, 0.11468599999999995)\n",
      "(8, 5, 0.017697000000000074)\n",
      "(8, 6, 0.002670999999999868)\n",
      "(8, 7, 0.0021979999999999222)\n",
      "(8, 8, 0.0021130000000000315)\n",
      "(5, 12, 0.5010000000000001)\n",
      "(6, 12, 0.44318599999999997)\n",
      "(3, 12, 0.43567400000000006)\n",
      "(6, 10, 0.04761800000000005)\n",
      "(5, 13, 0.07258300000000006)\n",
      "(9, 4, 0.014069999999999916)\n",
      "(8, 9, 0.30280700000000005)\n",
      "(6, 0, 0.003149999999999986)\n",
      "(6, 13, 0.07363700000000006)\n",
      "(7, 0, 0.0027610000000000134)\n",
      "(9, 5, 0.016091999999999995)\n",
      "(7, 9, 0.13327699999999998)\n",
      "(7, 12, 0.3675419999999999)\n",
      "(8, 13, 0.07350400000000001)\n",
      "(9, 6, 0.0022039999999999837)\n",
      "(9, 0, 0.0029569999999999874)\n",
      "(7, 1, 0.0012689999999999646)\n",
      "(9, 7, 0.0016639999999999988)\n",
      "(9, 1, 0.001040000000000041)\n",
      "(8, 10, 0.05620400000000014)\n",
      "(9, 8, 0.00303500000000001)\n",
      "(8, 11, 0.003960999999999881)\n",
      "(4, 12, 0.45408700000000013)\n",
      "(7, 13, 0.07401599999999986)\n",
      "(7, 10, 0.05172399999999999)\n",
      "(8, 0, 0.0029520000000000657)\n",
      "(8, 1, 0.0010999999999998789)\n",
      "(7, 11, 0.0026239999999999597)\n",
      "(9, 2, 0.07269200000000009)\n",
      "(9, 9, 0.13490500000000005)\n",
      "(9, 3, 0.0017080000000000428)\n",
      "(4, 13, 0.06931200000000004)\n",
      "(8, 2, 0.06783500000000009)\n",
      "(9, 10, 0.05314799999999997)\n",
      "(9, 11, 0.0024399999999999977)\n",
      "(8, 12, 0.522251)\n",
      "(9, 12, 0.7536550000000002)\n",
      "(9, 13, 0.04000500000000029)\n"
     ]
    }
   ],
   "source": [
    "#Train SuperLearner model\n",
    "SL = SuperLearner()\n",
    "SL.parallelfit(X_train, Y_train, models = modelsfull, k = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MSE for', <class 'sklearn.linear_model.logistic.LogisticRegression'>, 0.46305744520030234)\n",
      "('MSE for', <class 'sklearn.linear_model.coordinate_descent.Lasso'>, 0.39561550408008711)\n",
      "('MSE for', <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 0.46412824389014867)\n",
      "('MSE for', <class 'sklearn.naive_bayes.GaussianNB'>, 0.47001763668430335)\n",
      "('MSE for', <class 'sklearn.naive_bayes.MultinomialNB'>, 0.45716805240614766)\n",
      "('MSE for', <class 'sklearn.svm.classes.SVC'>, 0.46466364323507181)\n",
      "('MSE for', <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 0.46359284454522548)\n",
      "('MSE for', <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 0.46359284454522548)\n",
      "('MSE for', <class 'sklearn.linear_model.ridge.Ridge'>, 0.42664546741461207)\n",
      "('MSE for', <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 0.46359284454522548)\n",
      "('MSE for', <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>, 0.45177434015958928)\n",
      "('MSE for', <class 'sklearn.linear_model.base.LinearRegression'>, 0.42666462240076891)\n",
      "('MSE for', <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 0.46359284454522548)\n",
      "('MSE for', <class 'sklearn.ensemble.forest.RandomForestRegressor'>, 0.44526832955404388)\n",
      "('MSE for SuperLearner', 0.002862003318939558)\n",
      "Classification accuracy of 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdougal/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:429: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/mdougal/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:436: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "#Make predictions and classifications with trained model \n",
    "y_hat_SL_train = SL.predict(X_train, Y_train)\n",
    "classified_train, right_traing = SL.classify(y_hat_SL_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MSE for', <class 'sklearn.linear_model.logistic.LogisticRegression'>, 0.46145124716553287)\n",
      "('MSE for', <class 'sklearn.linear_model.coordinate_descent.Lasso'>, 0.38921260603632096)\n",
      "('MSE for', <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 0.45930964978584027)\n",
      "('MSE for', <class 'sklearn.naive_bayes.GaussianNB'>, 0.46359284454522548)\n",
      "('MSE for', <class 'sklearn.naive_bayes.MultinomialNB'>, 0.44860166288737718)\n",
      "('MSE for', <class 'sklearn.svm.classes.SVC'>, 0.47001763668430335)\n",
      "('MSE for', <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 0.46573444192491814)\n",
      "('MSE for', <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 0.46359284454522548)\n",
      "('MSE for', <class 'sklearn.linear_model.ridge.Ridge'>, 0.41994432064648757)\n",
      "('MSE for', <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 0.46359284454522548)\n",
      "('MSE for', <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>, 0.45015067732203484)\n",
      "('MSE for', <class 'sklearn.linear_model.base.LinearRegression'>, 0.41996741094653844)\n",
      "('MSE for', <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 0.45716805240614766)\n",
      "('MSE for', <class 'sklearn.ensemble.forest.RandomForestRegressor'>, 0.42946208112874784)\n",
      "('MSE for SuperLearner', 0.016574648681738476)\n",
      "Classification accuracy of 0.984126984127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdougal/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:429: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/mdougal/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:436: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "y_hat_SL= SL.predict(X_test, Y_test) #Makes predictions, Prints MSE for each model, final value is ensemble which has by far the lowest MSE \n",
    "classified, right = SL.classify(y_hat_SL, Y_test) #Classifies using SuperLearner and .5 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "def model_summary(model, X_test, Y_test):\n",
    "    Y_hat = model.predict(X_test)\n",
    "    Y_hat_classified = Y_hat >= .5\n",
    "    print(classification_report(Y_test, Y_hat_classified))\n",
    "    c_matrix = confusion_matrix(Y_test, Y_hat_classified)\n",
    "    print c_matrix\n",
    "    print(np.diag(c_matrix).sum() / float(c_matrix.sum()))\n",
    "    return np.diag(c_matrix).sum() / float(c_matrix.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.99      0.99        80\n",
      "        1.0       0.98      0.98      0.98        46\n",
      "\n",
      "avg / total       0.98      0.98      0.98       126\n",
      "\n",
      "[[79  1]\n",
      " [ 1 45]]\n",
      "0.984126984127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98412698412698407"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assessing precision, recall for SuperLearner\n",
    "model_summary(SL, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.99      0.98        80\n",
      "        1.0       0.98      0.96      0.97        46\n",
      "\n",
      "avg / total       0.98      0.98      0.98       126\n",
      "\n",
      "[[79  1]\n",
      " [ 2 44]]\n",
      "0.97619047619\n",
      "<class 'sklearn.linear_model.coordinate_descent.Lasso'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.99      0.98        80\n",
      "        1.0       0.98      0.93      0.96        46\n",
      "\n",
      "avg / total       0.97      0.97      0.97       126\n",
      "\n",
      "[[79  1]\n",
      " [ 3 43]]\n",
      "0.968253968254\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.99      0.98        80\n",
      "        1.0       0.98      0.93      0.96        46\n",
      "\n",
      "avg / total       0.97      0.97      0.97       126\n",
      "\n",
      "[[79  1]\n",
      " [ 3 43]]\n",
      "0.968253968254\n",
      "<class 'sklearn.naive_bayes.GaussianNB'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.99      0.99        80\n",
      "        1.0       0.98      0.98      0.98        46\n",
      "\n",
      "avg / total       0.98      0.98      0.98       126\n",
      "\n",
      "[[79  1]\n",
      " [ 1 45]]\n",
      "0.984126984127\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.87      0.95      0.91        80\n",
      "        1.0       0.90      0.76      0.82        46\n",
      "\n",
      "avg / total       0.88      0.88      0.88       126\n",
      "\n",
      "[[76  4]\n",
      " [11 35]]\n",
      "0.880952380952\n",
      "<class 'sklearn.svm.classes.SVC'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.95      0.97        80\n",
      "        1.0       0.92      0.98      0.95        46\n",
      "\n",
      "avg / total       0.96      0.96      0.96       126\n",
      "\n",
      "[[76  4]\n",
      " [ 1 45]]\n",
      "0.960317460317\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.97      0.98        80\n",
      "        1.0       0.96      0.98      0.97        46\n",
      "\n",
      "avg / total       0.98      0.98      0.98       126\n",
      "\n",
      "[[78  2]\n",
      " [ 1 45]]\n",
      "0.97619047619\n",
      "<class 'sklearn.tree.tree.DecisionTreeRegressor'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.97      0.97        80\n",
      "        1.0       0.96      0.96      0.96        46\n",
      "\n",
      "avg / total       0.97      0.97      0.97       126\n",
      "\n",
      "[[78  2]\n",
      " [ 2 44]]\n",
      "0.968253968254\n",
      "<class 'sklearn.linear_model.ridge.Ridge'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.99      0.98        80\n",
      "        1.0       0.98      0.93      0.96        46\n",
      "\n",
      "avg / total       0.97      0.97      0.97       126\n",
      "\n",
      "[[79  1]\n",
      " [ 3 43]]\n",
      "0.968253968254\n",
      "<class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.99      0.99        80\n",
      "        1.0       0.98      0.98      0.98        46\n",
      "\n",
      "avg / total       0.98      0.98      0.98       126\n",
      "\n",
      "[[79  1]\n",
      " [ 1 45]]\n",
      "0.984126984127\n",
      "<class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.96      0.97        80\n",
      "        1.0       0.94      0.96      0.95        46\n",
      "\n",
      "avg / total       0.96      0.96      0.96       126\n",
      "\n",
      "[[77  3]\n",
      " [ 2 44]]\n",
      "0.960317460317\n",
      "<class 'sklearn.linear_model.base.LinearRegression'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.99      0.98        80\n",
      "        1.0       0.98      0.93      0.96        46\n",
      "\n",
      "avg / total       0.97      0.97      0.97       126\n",
      "\n",
      "[[79  1]\n",
      " [ 3 43]]\n",
      "0.968253968254\n",
      "<class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.99      0.97        80\n",
      "        1.0       0.98      0.91      0.94        46\n",
      "\n",
      "avg / total       0.96      0.96      0.96       126\n",
      "\n",
      "[[79  1]\n",
      " [ 4 42]]\n",
      "0.960317460317\n",
      "<class 'sklearn.ensemble.forest.RandomForestRegressor'>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.99      0.98        80\n",
      "        1.0       0.98      0.96      0.97        46\n",
      "\n",
      "avg / total       0.98      0.98      0.98       126\n",
      "\n",
      "[[79  1]\n",
      " [ 2 44]]\n",
      "0.97619047619\n"
     ]
    }
   ],
   "source": [
    "#Same assessments for each of the underlying models\n",
    "for model in SL.fittedmodels:\n",
    "    print type(model)\n",
    "    model_summary(model, X_test, Y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdougal/anaconda2/lib/python2.7/site-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The axisbg attribute was deprecated in version 2.0. Use facecolor instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbcd4298fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting ROC Curve\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_hat_SL)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "fig, ax = plt.subplots(subplot_kw=dict(axisbg='#EEEEEE'))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbcd42a2390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting precision-Recall Curve\n",
    "precision, recall, threshold = precision_recall_curve(Y_test, y_hat_SL)\n",
    "\n",
    "average_precision = average_precision_score(Y_test, y_hat_SL)\n",
    "fig, ax = plt.subplots(subplot_kw=dict(axisbg='#EEEEEE'))\n",
    "lw = 2\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(recall, precision, lw=lw, color='navy',\n",
    "         label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall AUC={0:0.2f}'.format(average_precision))\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>cancer</td>      <th>  R-squared:         </th> <td>   0.847</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.845</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   382.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 07 Jul 2017</td> <th>  Prob (F-statistic):</th> <td>2.62e-246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:47:36</td>     <th>  Log-Likelihood:    </th> <td>  158.51</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   630</td>      <th>  AIC:               </th> <td>  -297.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   620</td>      <th>  BIC:               </th> <td>  -252.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     9</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>         <td>   -0.2485</td> <td>    0.017</td> <td>  -14.562</td> <td> 0.000</td> <td>   -0.282    -0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clump_thickness</th>   <td>    0.0342</td> <td>    0.004</td> <td>    9.358</td> <td> 0.000</td> <td>    0.027     0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>size_uniformity</th>   <td>    0.0236</td> <td>    0.007</td> <td>    3.577</td> <td> 0.000</td> <td>    0.011     0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shape_uniformity</th>  <td>    0.0129</td> <td>    0.006</td> <td>    2.034</td> <td> 0.042</td> <td>    0.000     0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marginal</th>          <td>    0.0091</td> <td>    0.004</td> <td>    2.240</td> <td> 0.025</td> <td>    0.001     0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>single_epithelial</th> <td>    0.0096</td> <td>    0.005</td> <td>    1.805</td> <td> 0.071</td> <td>   -0.001     0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bare_nucleoli</th>     <td>    0.0456</td> <td>    0.003</td> <td>   13.798</td> <td> 0.000</td> <td>    0.039     0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bland_chromatin</th>   <td>    0.0173</td> <td>    0.005</td> <td>    3.314</td> <td> 0.001</td> <td>    0.007     0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>normal_nucleoli</th>   <td>    0.0181</td> <td>    0.004</td> <td>    4.791</td> <td> 0.000</td> <td>    0.011     0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mitoses</th>           <td>    0.0025</td> <td>    0.005</td> <td>    0.489</td> <td> 0.625</td> <td>   -0.007     0.012</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>87.627</td> <th>  Durbin-Watson:     </th> <td>   1.756</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 357.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.568</td> <th>  Prob(JB):          </th> <td>2.42e-78</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.511</td> <th>  Cond. No.          </th> <td>    28.0</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 cancer   R-squared:                       0.847\n",
       "Model:                            OLS   Adj. R-squared:                  0.845\n",
       "Method:                 Least Squares   F-statistic:                     382.2\n",
       "Date:                Fri, 07 Jul 2017   Prob (F-statistic):          2.62e-246\n",
       "Time:                        16:47:36   Log-Likelihood:                 158.51\n",
       "No. Observations:                 630   AIC:                            -297.0\n",
       "Df Residuals:                     620   BIC:                            -252.6\n",
       "Df Model:                           9                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "-------------------------------------------------------------------------------------\n",
       "Intercept            -0.2485      0.017    -14.562      0.000        -0.282    -0.215\n",
       "clump_thickness       0.0342      0.004      9.358      0.000         0.027     0.041\n",
       "size_uniformity       0.0236      0.007      3.577      0.000         0.011     0.037\n",
       "shape_uniformity      0.0129      0.006      2.034      0.042         0.000     0.025\n",
       "marginal              0.0091      0.004      2.240      0.025         0.001     0.017\n",
       "single_epithelial     0.0096      0.005      1.805      0.071        -0.001     0.020\n",
       "bare_nucleoli         0.0456      0.003     13.798      0.000         0.039     0.052\n",
       "bland_chromatin       0.0173      0.005      3.314      0.001         0.007     0.028\n",
       "normal_nucleoli       0.0181      0.004      4.791      0.000         0.011     0.026\n",
       "mitoses               0.0025      0.005      0.489      0.625        -0.007     0.012\n",
       "==============================================================================\n",
       "Omnibus:                       87.627   Durbin-Watson:                   1.756\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              357.434\n",
       "Skew:                           0.568   Prob(JB):                     2.42e-78\n",
       "Kurtosis:                       6.511   Cond. No.                         28.0\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regression analysis, OLS for ease of interpretation\n",
    "import statsmodels.formula.api as sm\n",
    "sm.ols(formula = 'cancer ~ clump_thickness + size_uniformity + shape_uniformity + marginal + single_epithelial + bare_nucleoli + bland_chromatin + normal_nucleoli + mitoses', data = df).fit().summary()\n",
    "#Everything but  mitoses significant predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01,\n",
       "       -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01,\n",
       "       -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.  , -0.  ,\n",
       "       -0.  , -0.  , -0.  , -0.  , -0.  , -0.  , -0.  , -0.  , -0.  ,\n",
       "       -0.  , -0.  , -0.  , -0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "        0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.01,  0.01,  0.01,  0.01,\n",
       "        0.01,  0.01,  0.01,  0.01,  0.01,  0.01,  0.01,  0.01,  0.01,\n",
       "        0.02,  0.02,  0.02,  0.02,  0.02,  0.02,  0.02,  0.03,  0.03,\n",
       "        0.04,  0.06,  0.07,  0.09,  0.09,  0.22,  0.23,  0.3 ,  0.35,\n",
       "        0.79,  0.8 ,  0.81,  0.82,  0.84,  0.84,  0.88,  0.88,  0.9 ,\n",
       "        0.91,  0.91,  0.92,  0.93,  0.93,  0.93,  0.94,  0.94,  0.94,\n",
       "        0.94,  0.94,  0.95,  0.95,  0.96,  0.97,  0.97,  0.97,  0.97,\n",
       "        0.97,  0.98,  0.98,  0.98,  0.98,  0.98,  0.98,  0.99,  0.99,\n",
       "        1.  ,  1.01,  1.01,  1.01,  1.04,  1.05,  1.05,  1.05,  1.07])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_SL.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'> -0.0\n",
      "<class 'sklearn.linear_model.coordinate_descent.Lasso'> 0.0749086443287\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'> 0.00609408203096\n",
      "<class 'sklearn.naive_bayes.GaussianNB'> 0.0\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'> -0.00773946777131\n",
      "<class 'sklearn.svm.classes.SVC'> 0.197711514268\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'> 0.0\n",
      "<class 'sklearn.tree.tree.DecisionTreeRegressor'> -0.0757063472326\n",
      "<class 'sklearn.linear_model.ridge.Ridge'> 0.0\n",
      "<class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'> 0.419455698953\n",
      "<class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'> 0.0\n",
      "<class 'sklearn.linear_model.base.LinearRegression'> 0.084831050089\n",
      "<class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'> 0.0\n",
      "<class 'sklearn.ensemble.forest.RandomForestRegressor'> 0.293871837393\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(SL.fittedmodels):\n",
    "    print type(model), SL.SL.coef_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SuperLearner' object has no attribute 'ols'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-cd2e21cfe982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SuperLearner' object has no attribute 'ols'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
